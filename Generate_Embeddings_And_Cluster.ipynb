{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import colorcet\n",
    "\n",
    "from bokeh.plotting import show, output_file, save\n",
    "import bokeh.io\n",
    "\n",
    "# Uncomment if you want to see bokeh output in the notebook\n",
    "# Warning that this can increase the filesize quite a bit after visualization!\n",
    "# bokeh.io.output_notebook()\n",
    "\n",
    "# If you have the latest development UMAP, use this instead of the packaged plot\n",
    "#from umap import plot\n",
    "from plot import plot\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all post titles\n",
    "\n",
    "Extract the post titles from the provided database.  We write them to a file so that the unmodified BERT repository can generate embeddings for all of them.  Only thing to be aware of is that the author and post title are aligned by index (i.e. line number), so any changes to the database will break that alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by language\n",
    "Since we're using an English BERT model we should first separate the post titles by language using spaCy en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found filtered submissions!  Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def is_likely_french(row):\n",
    "    doc = nlp(row[\"title\"])\n",
    "    if doc._.language[\"language\"] == \"fr\" and doc._.language[\"score\"] > 0.99:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if not os.path.exists(\"./data/filtered_submission_titles.txt\"):\n",
    "    print(\"Submissions not yet filtered.  Filtering...\")\n",
    "    data = pd.read_feather('./data/reddit_combined_small.feather')\n",
    "    data['created_utc_dt'] = pd.to_datetime(data['created_utc'])\n",
    "    submissions = data[data[\"item_type\"] == \"submission\"]\n",
    "    #submissions[\"title\"].to_csv(\"./post_titles.txt\", index=False)\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "    \n",
    "    submissions = submissions[submissions[\"subreddit\"].isin(['britishcolumbia', 'Quebec', 'alberta', 'canadaguns', 'CanadianForces', 'Edmonton', 'CanadaPolitics', 'canada', 'vancouver', 'onguardforthee', 'metacanada', 'ontario'])]\n",
    "    submissions = submissions[~submissions.apply(is_likely_french, axis=1)]\n",
    "    submissions.reset_index(drop=True).to_feather(\"./data/filter_submissions.feather\")\n",
    "    submissions[\"title\"].to_csv(\"./data/filtered_submission_titles.txt\", header=None, index=False)\n",
    "else:\n",
    "    print(\"Found filtered submissions!  Loading...\")\n",
    "    submissions = pd.read_feather(\"./data/filter_submissions.feather\")\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "canada             47273\n",
       "vancouver          31995\n",
       "metacanada         26890\n",
       "CanadaPolitics     15621\n",
       "Edmonton           14024\n",
       "onguardforthee     11257\n",
       "ontario            11224\n",
       "canadaguns          8221\n",
       "alberta             6944\n",
       "CanadianForces      5360\n",
       "britishcolumbia     3980\n",
       "Quebec              3025\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.groupby(\"subreddit\").count()[\"author\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BERT embeddings\n",
    "\n",
    "This will be much faster if you have GPUs available.  **Note that we have already generated these for this particular dataset, so we can skip down to the clustering and plotting steps unless we have new data we want to process.**  Uncomment the lines if you have a base TensorFlow environment ready to go, or paste them in a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/google-research/bert.git\n",
    "#!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "#!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features.  You'll need to either have a BERT-compatible TensorFlow installed into your base environment, or run this in a separate terminal.  This is a pretty space-inefficient way to do this (generating the full embedding requires ~25GB of disk space, of which most is discarded when we filter to just the CLS tokens), but it requires the least custom code.  Modifying the \"extract_features\" script to just return CLS tokens can bypass this if disk space is an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python bert/extract_features.py \\\n",
    "#  --input_file=./data/filtered_submission_titles.txt \\\n",
    "#  --output_file=./data/filtered_submission_title_embeddings.jsonl \\\n",
    "#  --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "#  --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "#  --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#  --layers=-1 \\\n",
    "#  --max_seq_length=128 \\\n",
    "#  --batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try running it on the suspicious comments from the old transparency report\n",
    "# !python bert/extract_features.py \\\n",
    "#   --input_file=./data/suspicious_submission_titles.txt \\\n",
    "#   --output_file=./data/suspicious_titles.jsonl \\\n",
    "#   --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "#   --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "#   --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#   --layers=-1 \\\n",
    "#   --max_seq_length=128 \\\n",
    "#   --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to just the CLS embeddings (and save them).  **Start here if you already have generated embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found suspicious CLS embeddings.  Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./data/suspicious_titles_cls.json\"):\n",
    "    print(\"Suspicious CLS embeddings do not exist.  Generating...\")\n",
    "    lines = pd.read_json(\"./data/suspicious_titles.jsonl\", chunksize=100, lines=True)\n",
    "\n",
    "    cls_tokens = []\n",
    "\n",
    "    for chunk in lines:\n",
    "        for it, line in chunk.iterrows():\n",
    "            cls_tokens.append(line[\"features\"][0]['layers'][0]['values'])\n",
    "\n",
    "    susp_df = pd.DataFrame(cls_tokens)\n",
    "    susp_df.to_json(\"suspicious_titles_cls.json\")\n",
    "else:\n",
    "    print(\"Found suspicious CLS embeddings.  Loading...\")\n",
    "    susp_df = pd.read_json(\"./data/suspicious_titles_cls.json\")\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just want the \\[CLS\\] embedding from the last layer, so let's extract them and save separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found submission CLS embeddings.  Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./data/filtered_submission_title_embeddings_cls.json\"):\n",
    "    print(\"Submission CLS embeddings do not exist.  Generating...\")\n",
    "    lines = pd.read_json(\"./data/filtered_submission_title_embeddings.jsonl\", chunksize=100, lines=True)\n",
    "\n",
    "    cls_tokens = []\n",
    "\n",
    "    for chunk in lines:\n",
    "        for it, line in chunk.iterrows():\n",
    "            cls_tokens.append(line[\"features\"][0]['layers'][0]['values'])\n",
    "\n",
    "    tokens_df = pd.DataFrame(cls_tokens)\n",
    "    tokens_df.to_json(\"./data/filtered_submission_title_embeddings_cls.json\")\n",
    "else:\n",
    "    print(\"Found submission CLS embeddings.  Loading...\")\n",
    "    tokens_df = pd.read_json(\"./data/filtered_submission_title_embeddings_cls.json\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label source of each data point correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_labels = pd.DataFrame(['canada' for c in range(tokens_df.shape[0])])\n",
    "source_labels = pd.concat([source_labels, pd.DataFrame(['suspicious' for c in range(susp_df.shape[0])])])\n",
    "source_labels.reset_index(drop=True, inplace=True)\n",
    "tokens_df = pd.concat([tokens_df, susp_df])\n",
    "\n",
    "suspicious_submissions = pd.read_csv(\"./data/submissions.csv\")\n",
    "suspicious_submissions = suspicious_submissions.rename(columns={\"subreddit_name_prefixed\": \"subreddit\", \"id\": \"full_id\", \"author.name\": \"author\"})\n",
    "submissions = pd.concat([submissions, suspicious_submissions], sort=False)\n",
    "submissions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hover_df = submissions[[\"title\", \"subreddit\", \"author\", \"full_id\"]].reset_index(drop=True)\n",
    "submissions['source'] = source_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform UMAP and clustering on Post Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class lets us plot a saved embedding array with the built-in UMAP plotting functions\n",
    "# Useful to avoid having to pickle/unpickle the model (instead just saving/loading the embedding as Tensorboard compatible TSV)\n",
    "class UMAP_Facade:\n",
    "    embedding_ = None\n",
    "    def __init__(self, embedding=None):\n",
    "        self.embedding_ = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found UMAP embedding.  Loading...\n",
      "Done!\n",
      "Generating clusters...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Output as Tensorboard-friendly TSV\n",
    "if not os.path.exists(\"./data/posts_by_title_embedding_2d.tsv\"):\n",
    "    print(\"Did not find 2D UMAP embedding.  Generating...\")\n",
    "    # This usually takes 10 minutes or so on my machine\n",
    "    umap_model_2d = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=0, low_memory=True).fit(tokens_df)\n",
    "    \n",
    "    umap_df_2d = pd.DataFrame(umap_model.embedding_)\n",
    "    umap_df_2d.to_csv(\"./data/posts_by_title_embedding_2d.tsv\", sep='\\t', index=False, header=False)\n",
    "    hover_df.to_csv(\"./data/posts_by_title_embedding_labels.tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    print(\"Found UMAP embedding.  Loading...\")\n",
    "    umap_df_2d = pd.read_csv(\"./data/posts_by_title_embedding_2d.tsv\", sep='\\t', header=None)\n",
    "    hover_df = pd.read_csv(\"./data/posts_by_title_embedding_labels.tsv\", sep='\\t')\n",
    "    umap_model_2d = UMAP_Facade(umap_df_2d.to_numpy())\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Generating clusters...\")\n",
    "cluster_2d = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=15, metric='euclidean', cluster_selection_method='eom').fit(umap_model_2d.embedding_)\n",
    "cluster_labels_2d = pd.Series(cluster_2d.labels_)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found UMAP embedding.  Loading...\n",
      "Done!\n",
      "Generating clusters...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Output as Tensorboard-friendly TSV\n",
    "if not os.path.exists(\"./data/posts_by_title_embedding_3d.tsv\"):\n",
    "    print(\"Did not find 3D UMAP embedding.  Generating...\")\n",
    "    # This usually takes 10 minutes or so on my machine\n",
    "    umap_model_3d = umap.UMAP(n_neighbors=15, n_components=3, metric='cosine', random_state=0, low_memory=True).fit(tokens_df)\n",
    "    \n",
    "    umap_df_3d = pd.DataFrame(umap_model.embedding_)\n",
    "    umap_df_3d.to_csv(\"./data/posts_by_title_embedding_3d.tsv\", sep='\\t', index=False, header=False)\n",
    "    hover_df.to_csv(\"./data/posts_by_title_embedding_labels.tsv\", sep='\\t', index=False)\n",
    "else:\n",
    "    print(\"Found UMAP embedding.  Loading...\")\n",
    "    umap_df_3d = pd.read_csv(\"./data/posts_by_title_embedding_3d.tsv\", sep='\\t', header=None)\n",
    "    hover_df = pd.read_csv(\"./data/posts_by_title_embedding_labels.tsv\", sep='\\t')\n",
    "    umap_model_3d = UMAP_Facade(umap_df_3d.to_numpy())\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Generating clusters...\")\n",
    "cluster_3d = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=15, metric='euclidean', cluster_selection_method='eom').fit(umap_model_3d.embedding_)\n",
    "cluster_labels_3d = pd.Series(cluster_3d.labels_)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 2D interactive HTML Bokeh Plots\n",
    "If these don't interest you, you can skip these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme \"darkgreen\" looks nice\n",
    "def plot_interactive_bokeh(filename, model, labels, hover_data, point_size=4, width=1800, height=1800, theme=\"blue\"):\n",
    "    int_plot = plot.interactive(model, labels=labels, hover_data=hover_data, point_size=point_size, width=width, height=height, theme=theme)\n",
    "    save(int_plot , filename=filename, title=\"Bokeh Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_bokeh(\"./vis/post_plot_with_cluster.html\", umap_model_2d, labels=cluster_labels_2d, hover_data=hover_df, width=2000, height=2000)\n",
    "plot_interactive_bokeh(\"./vis/post_plot_with_authors.html\", umap_model_2d, labels=submissions[\"author\"], hover_data=hover_df)\n",
    "plot_interactive_bokeh(\"./vis/post_plot_with_subreddit.html\", umap_model_2d, labels=submissions[\"subreddit\"], hover_data=hover_df)\n",
    "plot_interactive_bokeh(\"./vis/post_plot_with_source.html\", umap_model_2d, labels=source_labels[0], hover_data=hover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot at smaller scale for visually-appealing density map\n",
    "# plot.points(umap_model_2d, width=1000, height=1000, theme=\"fire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Average Title Embedding by User\n",
    "\n",
    "Let's do some clustering on users with 10+ posts in the collected time range, based on their average [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_threshold = 10\n",
    "\n",
    "embeddings_by_author = defaultdict(list)\n",
    "for it, emb in tokens_df.iterrows():\n",
    "    embeddings_by_author[submissions.iloc[it][\"author\"]].append(emb)\n",
    "\n",
    "# A bit inefficient, since this used to live in separate notebooks\n",
    "    \n",
    "active_users = []\n",
    "avg_embeddings = []\n",
    "embedding_matrix = []\n",
    "    \n",
    "for user, emb_list in embeddings_by_author.items():\n",
    "    if len(emb_list) >= post_threshold:\n",
    "        active_users.append((user, emb_list))\n",
    "\n",
    "for u, emb_list in active_users:\n",
    "    avg_emb = np.zeros(768)\n",
    "    \n",
    "    for emb in emb_list:\n",
    "        avg_emb += np.array(emb)\n",
    "        \n",
    "    avg_emb /= len(emb_list)\n",
    "    avg_embeddings.append((u, avg_emb))\n",
    "    \n",
    "for e in avg_embeddings:\n",
    "    embedding_matrix.append(e[1])\n",
    "    \n",
    "emb_df = pd.DataFrame(embedding_matrix)\n",
    "avg_emb_df = pd.DataFrame(avg_embeddings, columns=[\"author\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange the labels in the right order, grab some example post titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_by_active_users = submissions[submissions[\"author\"].isin(avg_emb_df[\"author\"].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_samples = {}\n",
    "\n",
    "for author in avg_emb_df[\"author\"]:\n",
    "    hover_samples[author] = []\n",
    "    for posttext in subs_by_active_users[subs_by_active_users[\"author\"] == author][\"title\"][:5]:\n",
    "        hover_samples[author].append(posttext)\n",
    "        \n",
    "hover_df_sub = pd.DataFrame(hover_samples).T\n",
    "hover_df_sub = hover_df_sub.reset_index()\n",
    "hover_df_sub.columns = [\"author\", \"post1\", \"post2\", \"post3\", \"post4\", \"post5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce these average user representations into 2D and 3D visualizable spaces, and cluster with HDBSCAN on both EOM and Leaf settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 216 ms, total: 12.9 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "umap_model_avg_2d = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=0).fit(emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 s, sys: 43 ms, total: 8.23 s\n",
      "Wall time: 7.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "umap_model_avg_3d = umap.UMAP(n_neighbors=15, n_components=3, metric='cosine', random_state=0).fit(emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_user_df_2d = pd.DataFrame(umap_model_avg_2d.embedding_)\n",
    "umap_user_df_3d = pd.DataFrame(umap_model_avg_3d.embedding_)\n",
    "\n",
    "umap_user_df_2d.to_csv(\"./data/users_2d.tsv\", sep='\\t', index=False, header=False)\n",
    "umap_user_df_3d.to_csv(\"./data/users_3d.tsv\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_save_labels(embedding, mode, dimension, min_cluster_size=15, min_samples=15):\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean', cluster_selection_method=mode).fit(embedding)\n",
    "    cluster_labels = pd.Series(clusters.labels_)\n",
    "    cluster_labels.to_csv(f\"./data/users_hdbscan_labels_{dimension}d_{mode}_mc{min_cluster_size}_ms{min_samples}.tsv\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_save_labels(umap_model_avg_2d.embedding_, 'eom', '2')\n",
    "cluster_and_save_labels(umap_model_avg_3d.embedding_, 'eom', '3')\n",
    "cluster_and_save_labels(umap_model_avg_2d.embedding_, 'leaf', '2')\n",
    "cluster_and_save_labels(umap_model_avg_3d.embedding_, 'leaf', '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive bokeh plot for 2D user map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_avg_2d = pd.read_csv('./data/users_hdbscan_labels_2d_eom_mc15_ms15.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_bert = plot.interactive(umap_model_avg_2d, labels=cluster_labels_avg_2d[\"0\"], hover_data=hover_df_sub, point_size=4, width=800, height=800, theme=\"darkgreen\")\n",
    "save(agg_bert, filename=\"./vis/user_plot.html\", title=\"Bokeh Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source labels for user plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_authors = submissions[submissions['source'] == 'canada']['author'].unique()\n",
    "susp_authors = submissions[submissions['source'] == 'suspicious']['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_source_labels = pd.DataFrame()\n",
    "reduced_source_labels['author'] = avg_emb_df['author']\n",
    "\n",
    "raw_labels = ['canada' if x in canada_authors else 'suspicious' for x in reduced_source_labels['author']]\n",
    "\n",
    "hover_df_sub['source'] = raw_labels\n",
    "\n",
    "hover_df_sub.to_csv(\"./data/users_by_title_embeddings_source_labels.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hdd/dd/reddit-aaai2021/clean/vis/user_plot_with_source.html'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_bert_anno = plot.interactive(umap_model_avg_2d, labels=pd.Series(raw_labels), hover_data=hover_df_sub, point_size=4, width=800, height=800, theme=\"blue\")\n",
    "save(agg_bert_anno, filename=\"./vis/user_plot_with_source.html\", title=\"Bokeh Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMAP",
   "language": "python",
   "name": "umap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
